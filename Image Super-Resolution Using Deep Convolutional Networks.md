# Image Super-Resolution Using Deep Convolutional Networks

## ABSTRACT

我们提出了一种用于单幅图像超分辨率（SR）的深度学习方法。我们的方法直接学习低/高分辨率图像之间的端到端映射。该映射表示为一个深度卷积神经网络 (CNN)，它将低分辨率图像作为输入并输出高分辨率图像。我们进一步表明，传统的基于稀疏编码的 SR 方法也可以被视为深度卷积网络。但与单独处理每个组件的传统方法不同，我们的方法联合优化了所有层。我们的深度 CNN 结构轻巧，但展示了最先进的恢复质量，并实现了实际在线使用的快速速度。我们探索不同的网络结构和参数设置，以实现性能和速度之间的权衡。此外，我们扩展了我们的网络以同时处理三个颜色通道，并显示出更好的整体重建质量。

## INTRODUCTION

单图像超分辨率（SR）[20]，旨在从单个低分辨率图像中恢复高分辨率图像，是计算机视觉中的一个经典问题。这个问题本质上是不适定的，因为对于任何给定的低分辨率像素都存在多种解决方案。换句话说，它是一个欠定逆问题，其解不是唯一的。通常通过强先验信息约束解决方案空间来缓解此类问题。

### 基于稀疏编码的方法

首先，从输入图像中密集裁剪重叠块并进行预处理（例如，减去均值和归一化）。然后这些补丁由低分辨率字典编码。稀疏系数被传递到高分辨率字典中，用于重建高分辨率补丁。该管道由大多数基于示例的外部方法共享，它们特别注意学习和优化字典或构建有效的映射函数。但是，管道中的其余步骤很少在统一的优化框架中进行优化或考虑。

在本文中，我们展示了上述管道等效于深度卷积神经网络 ）。受这一事实的启发，我们考虑了一个卷积神经网络，它直接学习低分辨率和高分辨率图像之间的端到端映射。我们的方法从根本上不同于现有的外部基于示例的方法，因为我们的方法没有明确地学习字典或流形来建模补丁空间。这些是通过隐藏层隐式实现的。此外，补丁提取和聚合也被公式化为卷积层，因此参与优化。在我们的方法中，整个 SR 管道完全通过学习获得，几乎没有前/后处理。

SRCNN的优点：

1.设计结构考虑到了简单性，并且提供了更高的准确性。

2.在CPU上有着更高的准确性。

3.实验表明，当（i）更大和更多样化的数据集可用，和/或（ii）使用更大和更深的模型时，可以进一步提高网络的恢复质量。相反，更大的数据集/模型可能会给现有的基于示例的方法带来挑战。此外，所提出的网络可以同时处理三个通道的彩色图像，以实现改进的超分辨率性能。

总体而言，本研究的贡献主要体现在三个方面：

1.我们提出了一个用于图像超分辨率的全卷积神经网络。网络直接学习低分辨率和高分辨率图像之间的端到端映射，除了优化之外几乎没有前/后处理

2.我们在基于深度学习的 SR 方法和传统的基于稀疏编码的 SR 方法之间建立了关系。这种关系为网络结构的设计提供了指导

3.我们证明了深度学习在经典的超分辨率计算机视觉问题中很有用，并且可以实现良好的质量和速度。

## RELATED WORK

### Image Super-Resolution

根据图像先验，单图像超分辨率算法可以分为四种类型——预测模型、基于边缘的方法、图像统计方法和基于补丁（或基于示例）的方法。

### Convolutional Neural Networks

### Deep Learning for Image Restoration

已经有一些使用深度学习技术进行图像恢复的研究。多层感知器 (MLP)，其所有层都是全连接的（与卷积相反），适用于自然图像去噪  和去模糊后去噪 。与我们的工作更密切相关的是，卷积神经网络被应用于自然图像去噪 和去除噪声模式（污垢/雨水）。这些恢复问题或多或少是去噪驱动的。崔等人。建议在基于内部示例的方法的概念下将自动编码器网络嵌入到他们的超分辨率管道中。深度模型并非专门设计为端到端解决方案，因为级联的每一层都需要对自相似搜索过程和自动编码器进行独立优化。相反，所提出的 SRCNN 优化了端到端映射。此外，SRCNN 的速度更快。它不仅是一种定量优越的方法，而且是一种实用的方法。

## CONVOLUTIONAL NEURAL NETWORKS FOR SUPER-RESOLUTION

考虑一个单一的低分辨率图像，我们首先使用双三次插值将其放大到所需的大小，这是我们执行的唯一预处理3。让我们将插值图像表示为 Y。我们的目标是从 Y 中恢复出与地面实况高分辨率图像 X 尽可能相似的图像 F(Y)。为了便于表示，我们仍将 Y 称为“低分辨率”图像，尽管它的大小与 X 相同。我们希望学习映射 F ，它在概念上由三个操作组成：

#### Patch extraction and representation

此操作从低分辨率图像 Y 中提取（重叠）补丁，并将每个补丁表示为高维向量。这些向量包括一组特征图，其数量等于向量的维数。

#### Non-linear mapping

该操作将每个高维向量非线性地映射到另一个高维向量。每个映射向量在概念上都是高分辨率补丁的表示。这些向量包括另一组特征图。

#### Reconstruction

此操作聚合上述高分辨率的补丁表示以生成最终的高分辨率图像。该图像预计与地面实况 X 相似。

![image-20220717123721402](https://makedown-1304519375.cos.ap-beijing.myqcloud.com/makedown/image-20220717123721402.png)

给定一个低分辨率图像 Y，SRCNN 的第一个卷积层提取一组特征图。第二层将这些特征映射非线性映射到高分辨率补丁表示。最后一层将空间邻域内的预测结合起来，生成最终的高分辨率图像 F(Y)。

### Patch extraction and representation

#这一步等价于用卷积层提取图像的信息

图像恢复中一种流行的策略（例如，[1]）是密集提取块，然后用一组预训练的基（例如 PCA、DCT、Haar 等）表示它们。这相当于将图像通过一组卷积过滤器，每个过滤器都是一个基础。在我们的公式中，我们将这些基础的优化纳入网络的优化。形式上，我们的第一层表示为一个操作F1:
$$
F_{1}(\mathbf{Y})=\max \left(0, W_{1} * \mathbf{Y}+B_{1}\right)
$$
#这一块建议看论文原文，原文介绍更加清楚

### Non-linear mapping

第一层为每个补丁提取一个 n1 维特征。在第二个操作中，我们将这些 n1 维向量中的每一个映射成一个 n2 维向量。第二层的操作为：
$$
F_{2}(\mathbf{Y})=\max \left(0, W_{2} * F_{1}(\mathbf{Y})+B_{2}\right)
$$

### Reconstruction

在传统方法中，预测的重叠高分辨率补丁通常被平均以产生最终的完整图像。平均可以被认为是一组特征图上的预定义滤波器（其中每个位置都是高分辨率补丁的“扁平化”矢量形式）。受此启发，我们定义了一个卷积层来生成最终的高分辨率图像D
$$
F(\mathbf{Y})=W_{3} * F_{2}(\mathbf{Y})+B_{3}
$$
有趣的是，尽管上述三个操作是出于不同的直觉，但它们都导致了与卷积层相同的形式。我们将所有三个操作放在一起，形成一个卷积神经网络（图 2）。在这个模型中，所有的过滤权重和偏差都将被优化。尽管整体结构简洁，但我们的 SRCNN 模型是通过吸取超分辨率 的重大进展所产生的丰富经验精心开发的。

## Relationship to Sparse-Coding-Based Methods

#这一步其实就是介绍了运用卷积的方法可以替代基于稀疏编码的SR方法。

**典型的基本设置是 f1 = 9、f2 = 1、f3 = 5、n1 = 64 和 n2 = 32,n3=3**，这句话是很重要的

## Training

**LOSS函数(MSE)**
$$
L(\Theta)=\frac{1}{n} \sum_{i=1}^{n}\left\|F\left(\mathbf{Y}_{i} ; \Theta\right)-\mathbf{X}_{i}\right\|^{2}
$$
**评估指标**

**PSNR**
$$
\begin{array}{l}
P S N R=10 \times \log _{10}\left(\frac{\left(2^{n}-1\right)^{2}}{M S E}\right) \\
M S E=\frac{1}{m n} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1}\|I(i, j)-K(i, j)\|^{2}
\end{array}
$$

$$
P S N R=10 \cdot \log _{10}\left(\frac{M A X_{I}^{2}}{M S E}\right)=20 \cdot \log _{10}\left(\frac{M A X_{I}}{\sqrt{M S E}}\right)
$$

#其中MAXI：表示图像颜色的最大数值，8位采样点表示为255。

**SSIM**
$$
\begin{array}{c}
\operatorname{SSIM}(\mathbf{x}, \mathbf{y})=[l(\mathbf{x}, \mathbf{y})]^{\alpha}[c(\mathbf{x}, \mathbf{y})]^{\beta}[s(\mathbf{x}, \mathbf{y})]^{\gamma}, \\
l(\mathbf{x}, \mathbf{y})=\frac{2 \mu_{x} \mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}, \quad c(\mathbf{x}, \mathbf{y})=\frac{2 \sigma_{x} \sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}, s(\mathbf{x}, \mathbf{y})=\frac{\sigma_{x y}+C_{3}}{\sigma_{x} \sigma_{y}+C_{3}} 。
\end{array}
$$
**采用adam的方法将损失最小化**

**为了避免训练期间的边界效应，所有卷积层都没有填充**